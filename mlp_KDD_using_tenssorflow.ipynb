{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda env:py35]",
      "language": "python",
      "name": "conda-env-py35-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "my1stNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Janengh69/ipynb/blob/master/mlp_KDD_using_tenssorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2taF00OZawRc",
        "colab_type": "code",
        "outputId": "abfc580d-d509-4730-98ab-5a8404b7c7ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "%tensorflow_version 1.x\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwzTY9doePxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dir of total dataset\n",
        "total_dataset_dir = \"drive/My Drive/kdd99/kddcup.data_10_percent_corrected\"\n",
        "training_set_dir = \"drive/My Drive/kdd99/train.csv\"\n",
        "test_set_dir = \"drive/My Drive/kdd99/test.csv\"\n",
        "debug_set_dir = \"debug.csv\"\n",
        "neural_network_model_file = \"drive/My Drive/kdd99/model.ckpt\"\n",
        "\n",
        "#Parse total dataset\n",
        "\n",
        "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
        "\n",
        "num_features = [\n",
        "    \"duration\",\"protocol_type\",\"service\",\"flag\", \"src_bytes\",\n",
        "    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "    \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
        "    \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
        "    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\"label\"\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuV2SHFciC86",
        "colab_type": "code",
        "outputId": "8f5e4b42-ba1f-457d-844a-a0ecedf39e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Training set\n",
        "\n",
        "training_set = pandas.read_csv(training_set_dir, names=num_features)\n",
        "training_label = training_set[\"label\"]\n",
        "print(training_label)\n",
        "training_label = training_label.as_matrix()\n",
        "training_set.drop(['label'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "#Test set\n",
        "\n",
        "test_set = pandas.read_csv(test_set_dir, names = num_features)\n",
        "test_label = test_set[\"label\"]\n",
        "test_label = test_label.as_matrix()\n",
        "\n",
        "test_set.drop(['label'], axis=1, inplace=True)\n",
        "\n",
        "#Debug set\n",
        "\n",
        "#debug_set = pandas.read_csv(debug_set_dir,  names = num_features)\n",
        "#debug_labels = debug_set[\"label\"]\n",
        "#debug_labels = debug_labels.as_matrix()\n",
        "#debug_set.drop(['label'], axis=1, inplace=True)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0    18\n",
            "0.0    18\n",
            "0.0    18\n",
            "0.0     9\n",
            "0.0    11\n",
            "       ..\n",
            "0.0    18\n",
            "0.0    18\n",
            "0.0    18\n",
            "0.0    18\n",
            "0.0     9\n",
            "Name: label, Length: 345814, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSSTpJydiH2o",
        "colab_type": "code",
        "outputId": "9b35aaa9-e417-47f1-8a80-3e84e7b62446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "\n",
        "# Setup input and output\n",
        "input_set = training_set.as_matrix()\n",
        "print(input_set)\n",
        "# Add ones for bias and x inputs\n",
        "N, M = input_set.shape\n",
        "input_x = np.ones((N, M + 1))\n",
        "input_x[:, :-1] = input_set\n",
        "\n",
        "\n",
        "#Sizes\n",
        "epoch_num = 50\n",
        "dataset_size = training_set[\"duration\"].count()\n",
        "input_size = len(num_features)\n",
        "output_size = 23\n",
        "hidden_size = 500\n",
        "batch_size = 200000\n",
        "\n",
        "\n",
        "# Setup y outputs\n",
        "output_y = np.zeros((dataset_size, output_size), dtype=float)\n",
        "for j in range(dataset_size):\n",
        "    output_y[j, training_label[j]] = 1.0;\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.21538462 0.9        ... 0.         0.         0.81818182]\n",
            " [0.         0.21538462 0.9        ... 0.         0.         0.81818182]\n",
            " [0.         0.21538462 0.9        ... 0.         0.         0.81818182]\n",
            " ...\n",
            " [0.         0.21538462 0.9        ... 0.         0.         0.81818182]\n",
            " [0.         0.21538462 0.9        ... 0.         0.         0.81818182]\n",
            " [0.5        0.69230769 0.5        ... 0.         0.         0.40909091]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iMefAzSiKov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device(\"/gpu:0\"):\n",
        "    #Place holder\n",
        "    tf.reset_default_graph()\n",
        "    x = tf.placeholder(shape=(None , input_size), dtype=tf.float64, name='X')\n",
        "    y = tf.placeholder(shape=(None , output_size ), dtype=tf.float64, name='y')\n",
        "\n",
        "    # Randomize Weights\n",
        "    W1 = tf.Variable(tf.truncated_normal([input_size, hidden_size ], stddev=0.01, dtype=tf.float64), dtype=tf.float64)\n",
        "    W2 = tf.Variable(tf.truncated_normal([hidden_size, hidden_size ], stddev=0.01, dtype=tf.float64), dtype=tf.float64)\n",
        "    W3 = tf.Variable(tf.truncated_normal([hidden_size, output_size ], stddev=0.01, dtype=tf.float64), dtype=tf.float64)\n",
        "\n",
        "\n",
        "    # Forward propagation\n",
        "    A1 =  tf.nn.sigmoid (tf.matmul(x, W1))\n",
        "    A2 =  tf.nn.sigmoid  (tf.matmul(A1, W2))\n",
        "    yhat = (tf.matmul(A2, W3))\n",
        "\n",
        "      #Loss function\n",
        "      #loss = tf.reduce_sum(tf.square(yhat - y))\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = yhat, labels =y))\n",
        "\n",
        "\n",
        "    #Train\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate=0.007).minimize(loss)\n",
        "    tf.set_random_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyCBBKZiNFD",
        "colab_type": "code",
        "outputId": "7666c997-0abb-4ee8-f791-3add076ec0d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#sess = tf.Session()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    loss_plot = []\n",
        "    # for i in range(epoch_num):\n",
        "    #     print('EPOCH', i)\n",
        "\n",
        "    #     #Batch\n",
        "    #     for k in range(round(int(dataset_size)/batch_size)):\n",
        "    #         start = k*batch_size\n",
        "    #         end = start + batch_size\n",
        "    #         _, loss_val = sess.run([train_op, loss] , feed_dict={x: input_x[start:end,:], y: output_y[start:end,:]})\n",
        "\n",
        "    #     loss_plot.append(loss_val)\n",
        "    #     print(\"Loss: \", loss_val)\n",
        "\n",
        "    # saver = tf.train.Saver()\n",
        "    # # # Save data\n",
        "    # saver.save(sess, neural_network_model_file)\n",
        "    # #Plot\n",
        "    # plt.plot(loss_plot)\n",
        "    # plt.ylabel('Loss')\n",
        "    # plt.show()\n",
        "\n",
        "    # # Load data\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.import_meta_graph(neural_network_model_file+\".meta\")\n",
        "    saver.restore(sess, 'drive/My Drive/kdd99/model.ckpt')\n",
        "\n",
        "\n",
        "    #Test data\n",
        "\n",
        "    # Setup input and output\n",
        "    test_input_set = test_set.as_matrix()\n",
        "\n",
        "    # Add ones for bias and x inputs\n",
        "    N, M = test_input_set.shape\n",
        "    test_input_x = np.ones((N, M + 1))\n",
        "    test_input_x[:, :-1] = test_input_set\n",
        "\n",
        "    test_dataset_size = test_set[\"duration\"].count()\n",
        "\n",
        "    # Setup y outputs\n",
        "    test_output_y = np.zeros((test_dataset_size, output_size), dtype=float)\n",
        "    for j in range(test_dataset_size):\n",
        "        # print(test_output_y[j])\n",
        "        # print(test_label[j])\n",
        "        test_output_y[j, test_label[j]] = 1.0;\n",
        "\n",
        "    total_tests = 0\n",
        "    correct = 0\n",
        "    confusion = np.zeros((22, 22))\n",
        "    for k in range(round(int(test_dataset_size) / batch_size)):\n",
        "        start = k * batch_size\n",
        "        end = start + batch_size\n",
        "        test_ = test_output_y[start:end, :]\n",
        "        y_est_np = sess.run(yhat, feed_dict={x: test_input_x[start:end,:] , y: test_ })\n",
        "        batch_correct = [estimate.argmax(axis=0) == target.argmax(axis=0)\n",
        "                   for estimate, target in zip(y_est_np, test_ )]\n",
        "        total_tests = total_tests + len(batch_correct)\n",
        "        correct = correct + sum(batch_correct)\n",
        "        # print(correct)\n",
        "        # print(total_tests)\n",
        "        confusion = confusion + confusion_matrix(test_label,  np.argmax(y_est_np, axis=1)   )\n",
        "\n",
        "    \n",
        "    # print(test_input_x)\n",
        "    # print(test_output_y)\n",
        "    y_est_np = sess.run(yhat, feed_dict={x: test_input_x, y: test_output_y })\n",
        "\n",
        "    correct = [estimate.argmax(axis=0) == target.argmax(axis=0)\n",
        "                for estimate, target in zip(y_est_np, test_output_y )]\n",
        "\n",
        "    accuracy = 100 * sum(correct) / len(correct)\n",
        "    confusion = confusion_matrix(test_label,  np.argmax(y_est_np, axis=1)   )\n",
        "    #f1 = f1_score(test_label, np.argmax(y_est_np, axis=1))\n",
        "    stats = precision_recall_fscore_support(test_label , np.argmax(y_est_np, axis=1) , average='weighted')\n",
        "\n",
        "    # Calculate the prediction accuracy\n",
        "    #accuracy = 100*(correct/total_tests)\n",
        "    print('Network  accuracy: %.2f%%' % accuracy)\n",
        "    #print('Number of tests: %d' % total_tests)\n",
        "    print(stats)\n",
        "\n",
        "    df = pandas.DataFrame(confusion)\n",
        "    # print(df)\n",
        "    df.to_csv(\"confusion.csv\" )\n",
        "\n",
        "    # df = pandas.DataFrame(f1)\n",
        "    # df.to_csv(\"f1_score.csv\")\n",
        "\n",
        "    sess.close()\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/My Drive/kdd99/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Network  accuracy: 98.16%\n",
            "(0.9645493404674118, 0.9815798174175309, 0.9727698200029132, None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3utkXDjCr6TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(output_y[1])\n",
        "import json\n",
        "out_list = list()\n",
        "test_list = list()\n",
        "with open('drive/My Drive/kdd99/data.json', 'r') as file:\n",
        "  data = json.load(file)\n",
        "  for ind in np.argmax(y_est_np, axis=1):\n",
        "    out_list.append([number for number, name in data.items() if name==str(ind)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOb55elAYlTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error_list =list()\n",
        "with open('drive/My Drive/kdd99/error.txt', 'w+') as file:\n",
        "  for i in range(len(test_)):\n",
        "    if out_list[i] != [number for number, name in data.items() if name==str(test_label[i])]:\n",
        "      # print(out_list[i], [number for number, name in data.items() if name==str(test_label[i])])\n",
        "      file.write(\"\".join(out_list[i]) + \"  \" + \"\".join([number for number, name in data.items() if name==str(test_label[i])]) + '\\n') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}